% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/main.R
\encoding{UTF-8}
\name{pbbo}
\alias{pbbo}
\title{\code{pbbo}: prior by Bayesian optimisation}
\usage{
pbbo(
  model_name = "default",
  target_lcdf,
  target_lpdf,
  target_sampler,
  prior_predictive_sampler,
  param_set,
  covariate_values = NULL,
  discrepancy = "log_cvm",
  n_crs2_iters = 2000,
  n_internal_prior_draws = 50000,
  importance_method = "uniform",
  importance_args = list(uniform_lower = NULL, uniform_upper = NULL,
    gamma_sd_multiplier = 1.05, student_t_sd_multiplier = 1.05,
    surv_mixture_sd_multiplier = 1.05, surv_mixture_cont_frac = 0.95,
    surv_mixture_cens_times = rep(NULL, length(covariate_values))),
  n_internal_importance_draws = 5000,
  n_design_pad = 10,
  bayes_opt_batches = 1,
  bayes_opt_iters_per_batch = 300,
  bayes_opt_design_points_per_batch = min(4 * length(param_set$pars),
    bayes_opt_iters_per_batch),
  bayes_opt_print = FALSE,
  extra_objective_term = NULL,
  ...
)
}
\arguments{
\item{model_name}{String: name of the model/optimisation function}

\item{target_lcdf}{Function: Vectorised target log-CDF. Takes one argument, a
vector of points at which we wish to evaluate the log-CDF. This function
must accept \code{\link[Rmpfr]{mpfr}} points. This requirement can be
trivially handled by calling \code{as.numeric} on the vector of points, but
performance is improved by using high precision versions of functions
compatible with \code{\link[Rmpfr]{mpfr}} points. See
\code{\link[Rmpfr]{mpfr-class}} for a range of such functions.}

\item{target_lpdf}{Function: Vectorised target log-PDF (density/mass function
). Corresponds to \code{target_lcdf}, and is currently required by the
importance sampling algorithm.}

\item{target_sampler}{Function: Generates samples from the target
distribution. Takes one argument \code{n}, the desired number of samples.}

\item{prior_predictive_sampler}{Function: Generates samples from the prior
predictive distribution. Takes two arguments: \code{n}, the desired number
of samples; and \code{lambda}, a named vector whose names correspond to the
hyperparameters of interest (and defined by \code{param_set}).}

\item{param_set}{A \code{\link[ParamHelpers]{makeParamSet}}: Parameter set
corresponding to the hyperparameters of interest. Best understood by
inspecting the example. For an example with vectors of parameters, we could
take

\preformatted{param_set <- makeParamSet(
    makeNumericVectorParam(id = 'mu', len = 2, lower = -50, upper = 50),
    makeNumericParam(id = 'sigma', lower = 0, upper = 20)
)}

the elements of which would be accessible inside
\code{prior_predictive_sampler} as \code{c(lambda['mu1'], lambda['mu2'],
  lambda['sigma'])}.}

\item{covariate_values}{An optional vector or matrix/data.frame of covariate
values: \bold{including covariate values changes the signatures required
for} \code{target_lcdf}, \code{target_sampler}, \bold{and}
\code{prior_predictive_sampler}. Each of these function must take an
additional third argument, perhaps called \code{cov}, that dictates the
behaviour of the function at a specific covariate value, i.e. value in the
covariate vector or row in the covariate matrix.}

\item{discrepancy}{String (or Function): One of either \code{'log_cvm'} or
\code{'log_ad'} for Cramér–von Mises (default) or Anderson–Darling distance
respectively. The latter is currently unsupported as it is too numerically
difficult to compute accurately. Alternatively, one can supply a function
with the signature \code{function(ecdf_1, lcdf_2, points, weights)}, where
the first argument is the ECDF of the PPD, the second is the LCDF of the
target, \code{points} is a vector of evaluation points, and \code{weights}
is the corresponding vector of importance weights.}

\item{n_crs2_iters}{Numeric: Number of iterations to run the CRS2 algorithm
from \code{\link[nloptr]{nloptr}}. These are used to initialise the Bayes
Opt stages, as it can be better at finding global minima. It only handles
single objectives, so it does not include the \code{extra_objective_term}.}

\item{n_internal_prior_draws}{Numeric: Number of draws to generate from the
prior predictive distribution for the given value of \code{lambda} in each
of the optimisation iterations. More draws result in better estimates of
the ECDF. Particularly importance if \code{discrepancy = "log_ad"}.}

\item{importance_method}{String: Defaults to \code{'uniform'}. Specifies
which importance sampling method should be used. Options include \code{
  gamma_mixture} for data on the positive real numbers and \code{
  student_t_mixture} for data on the whole real line.}

\item{importance_args}{List: Additional optional arguments to the importance
functions. These should be in the form of \code{importance_param}, e.g.
\code{uniform_lower} to set the lower bound for the uniform importance
method. Possible arguments include:
\itemize{
\item{\code{uniform_lower}}
{Left as NULL (the default) the lower bound of the uniform importance
distribution is set to be \code{min(sample_from_target ,
  sample_from_current_prior_predictive)}. If you wish to set this lower bound
manually, perhaps if the draws from the prior predictive distribution have
a compact support, then set \code{uniform_lower} to a specific value}
\item{\code{uniform_upper}}
{Corresponding upper bound to \code{
  uniform_lower}} \item{\code{gamma_sd_multiplier}}{Constant with which to
multiply the estimated standard deviations for each of the mixture
components. Can help in ensuring the tails are sufficiently heavy.}
\item{\code{student_t_multiplier}}
{Same purpose as \code{
  gamma_sd_multiplier} but for the mixture of student-t densities
importance.}
}}

\item{n_internal_importance_draws}{Numeric: Number of draws to generate from
the importance distribution, and subsequently used to evaluate the
discrepancy integral.}

\item{n_design_pad}{Numeric: Number of additional samples added to designs
for numerical stability/robustness. These are drawn from a Latin hypercube.}

\item{bayes_opt_batches}{Numeric: Number of batches to run the Bayesian
optimisation algorithm for. Minimum 1.}

\item{bayes_opt_iters_per_batch}{Numeric: Number of iterations of Bayesian
optimisation to use per batch. Passed to
\code{\link[mlrMBO]{setMBOControlTermination}} as \code{iters}.}

\item{bayes_opt_design_points_per_batch}{Numeric: Number of points from
previous batch to carry forward into the next batch. Points are selected by
sampling, with weights proportional to the value of the objective function
at each point.}

\item{bayes_opt_print}{Boolean: if \code{TRUE}, print the progress/status
from \code{\link[mlrMBO]{mbo}}. Defaults to \code{FALSE}.}

\item{extra_objective_term}{Function: univariate real-value function that
takes \code{lambda} as an argument and the result is added on to the
objective function.}

\item{...}{Currently unused.}
}
\value{
A list of length \code{bayes_opt_batches}, with each element being a
\code{\link[mlrMBO]{MBOSingleObjResult}}.
}
\description{
\code{pbbo}: prior by Bayesian optimisation
}
\examples{
\dontrun{
library(pbbo)
library(mlrMBO)

target_lcdf <- function(x) {
  pnorm(x, mean = 2, sd = 0.5, log.p = TRUE)
}

target_sampler <- function(n) {
  rnorm(n = n, mean = 2, sd = 0.5)
}

prior_predictive_sampler <- function(n, lambda) {
  rnorm(n = n, mean = lambda['mu'], sd = lambda['sigma'])
}

param_set <- makeParamSet(
  makeNumericParam(id = 'mu', default = 0.2, lower = -50, upper = 50),
  makeNumericParam(id = 'sigma', lower = 0, upper = 20, default = 0.2)
)

pbbo(
  model_name = 'test_normal',
  target_lcdf = target_lcdf,
  target_sampler = target_sampler,
  prior_predictive_sampler = prior_predictive_sampler,
  discrepancy = 'cvm',
  param_set = param_set,
  n_internal_prior_draws = 750,
  n_internal_importance_draws = 200,
  bayes_opt_iters_per_batch = 50,
  bayes_opt_print = FALSE
)}
}
